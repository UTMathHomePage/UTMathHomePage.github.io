<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="UTMath: Math Evaluation with Unit Test
        via Reasoning-to-Coding Thoughts">
  <meta name="keywords" content="UTMath">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>UTMath: Math Evaluation with Unit Test
    via Reasoning-to-Coding Thoughts</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/shield.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">UTMath: Math Evaluation with Unit Test
            via Reasoning-to-Coding Thoughts</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Bo Yang<sup>1</sup>,</span>
            <span class="author-block">Qingping Yang<sup>2</sup>,</span>
            <span class="author-block">Yingwei Ma<sup>2</sup>,</span>
            <span class="author-block">Runtao Liu<sup>3</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>South China University of Technology,</span>
            <span class="author-block"><sup>2</sup>ReasonMind,</span>
            <span class="author-block"><sup>3</sup>Hong Kong University of Science and Technology</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">sdyangbo02 [at] mail [dot] scut [dot] edu [dot] cn </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">qingping95 [at] gmail [dot] com </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">yingwei [dot] ywma [at] gmail [dot] com </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">runtao219 [at] gmail [dot] com </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2411.07240" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UTMathGroup/UTMath" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Hugging Face Link -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/UTMath/UTMath" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">🤗</span>
                  <span>HFDataset UTMath</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/UTMath/UTMath_Train" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">🤗</span>
                  <span>HFDataset UTMath_Train</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://utmathhomepage.github.io/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">🚀</span>
                  <span>Home Page</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" style="margin-top: -60px; margin-bottom: -60px;">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
  <p align="center">
    <img alt="image" src="./images/overview.png">
  </p>
  <p><i>
    UTMath is a <strong>cutting-edge</strong> and comprehensive benchmark designed to evaluate the mathematical reasoning abilities of Large Language Models. It consists of <strong>1,053 problems</strong>, each with an average of <strong>68 test cases</strong>, ensuring that models <strong>genuinely solve the problems</strong> rather than merely recalling memorized answers.
  </i></p>
  <p><i>
    The Reasoning-to-Coding of Thoughts (RCoT) approach complements the UTMath Benchmark by encouraging LLMs to engage in explicit reasoning prior to generating code. RCoT significantly improves the efficiency and effectiveness of the solution, suggesting that it encourages the model to <strong>reason critically and find more efficient solutions</strong>.
  </i></p>
  
  
  <ul>
    <li><b>⚡️Multiple Case Validation</b>: Instead of using single cases that can be memorized, our questions are sequence-based, allowing numerous cases for validating true understanding.</li>
    <li><b>🔧Code Output</b>: UTMath requires large models to solve cases by generating code, aiming for general solutions rather than problem-specific ones, reflecting a closer alignment with intelligence.</li>
    <li><b>🏆Enhanced Reasoning</b>: Emphasizing reasoning allows large models to focus more on improving the quality of reasoning, thereby delivering higher-quality and more efficient solutions.</li>
    <li><b>🌐Modularity</b>: By separating reasoning from implementation, it becomes possible to control variables and mitigate the impact of differences in reasoning and coding capabilities across various large models.</li>
  </ul>
  </div>
  </div>
</section>

<section class="section" style="margin-top: -60px; margin-bottom: -60px;">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <p><b>[2024/11]</b>🚀🚀🚀: We released our <b>code</b>📝
      <p><b>[2024/11]</b>🚀🚀🚀: We released our benchmark <a href="https://github.com/UTMathGroup/UTMath/tree/main">UTMath</a>.</p>
      <p><b>[2025/01]</b>🚀🚀🚀: UTMath has already received over 390 downloads on Hugging Face.</p>
    </div>
  </div>
</section>
<section class="section" id="BibTeX" style="margin-top: -50px; margin-bottom: -50px;">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">💬 Citation</h2>
    <p>
      If you find our work interesting and meaningful, welcome to give a 🌟 to our repo and cite our paper.
    </p>
    <pre><code>@article{yang2024utmath,
      title={UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts},
      author={Yang, Bo and Yang, Qingping and Liu, Runtao},
      journal={arXiv preprint arXiv:2411.07240},
      year={2024}
    }</code></pre>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
        <h2 class="title is-3">🥰 Acknowledgement</h2>
          <p>
            <li>We sincerely thank the <a href="https://oeis.org/wiki/Welcome" >OEIS</a> for its tireless efforts and contributions to the advancement of mathematics and computer science.</li>
          </p>
          <p>
            <li>We are also grateful to <a href =https://github.com/openai/human-eval>HumanEval</a> for providing valuable code resources. </li>
          </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). 
            While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and generality. 
            This paper introduces the UTMath Benchmark, a robust evaluation framework designed to assess LLMs through extensive unit tests, with a focus on both the accuracy and generality of model responses. The benchmark comprises 1,053 cutting-edge problems spanning nine mathematical domains, with an average of 68 test cases per problem.
            Furthermore, we present the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to code generation, thereby facilitating the production of more sophisticated solutions and enhancing overall performance and efficiency. Furthermore, we also release the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🥇 Leaderboard</h2>
        <div class="content has-text-justified">
          <p>
            <li>The best model, GPT-4o, only solves 26.93% problems in our benchmark, demonstrate the difficulty of our benchmarks.</li>          
          </p>
          <p align="center">
            <img width="774" alt="image" src="./images/leaderboard.png">
            Pass Rate and Average Run Time of LLMs on UTMath. We listed the performance of eight large models using PoT(Program of Thoughts) and RCoT methods across a range of metrics. For o1-mini and o1-preview only Pass@1 data is currently available due to resource constraints. The average run time is calculated based on the problems solved by the PoT or RCoT methods. The efficiency is calculated as: (Avg.Runtime(PoT) - Avg.Runtime(RcoT)) / Avg.Runtime(RcoT).        
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🚠 Generation Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            <li>The benchmark comprises 1,053 cutting-edge problems spanning nine mathematical domains, with an average of 68 test cases per problem.</li>          
          </p>
          <p align="center">
            <img alt="image" src="./images/Benchmark_Construction.png">
            UTMath generation pipeline.After downloading 23,238 Principle Sequences from OEIS and cleaning the data, 1,053 usable sequences were obtained. Descriptions were standardized by adding background information and improving readability (highlighted in green). Hard cases were introduced to enhance discriminative capability, including terms from later positions to prevent simplistic algorithms from passing.        
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">📋 Dataset Statistics</h2>
        <div class="content has-text-justified">
          <p>
            <li>The benchmark comprises 1,053 cutting-edge problems spanning nine mathematical domains, with an average of 68 test cases per problem.</li>          
          </p>
          <p align="center">
            <img alt="image" src="./images/Dataset_Statistics.png">
            Comparison between UTMath and other benchmarks. UTMath offers a cutting-edge benchmark with a comprehensive set of 1,053 problems across multiple mathematical domains, providing a more accurate evaluation of LLMs' mathematical reasoning capabilities.        
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">📖 Case Study</h2>
        <div class="content has-text-justified">
          <p>
            <li>This is a qualitative analysis case study of UTMath and RCoT.</li>          
          </p>
          <p align="center">
            <img alt="image" src="./images/Case_Study.png">
            GPT-4o solves UTMath_948 by the PoT method, by the RCoT method, respectively. PoT simply performs brute-force solving, while RCoT involves deeper reasoning through Case merging after a classification discussion and the application of Euler's formula, providing a solution with lower time complexity.        
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">😎 Some interesting findings</h2>
        <div class="content has-text-justified">
          <p>
            We conducted a comprehensive study with 8 LLMs. Some of our key findings are summarized as follows:
          </p>
          <p>  
            <li>Modern LLMs perform poorly in Graph Theory, Group Theory, Geometry and Topology.</li>
          </p>
          <p align="center">
            <img width="782" alt="image" src="./images/performance_on_different_problems_categories.png">
            Performance on Different Problem Categories.(%) Categories are represented by abbreviations. NT: Number Theory; T.: Theory; DM: Discrete Mathematics; CM: Combinatorial Mathematics; GT: Geometry and Topology; PSE: Polynomial and Series Expansions; SN: Special Numbers; FL: Formal Languages.
          </p>
          <p>  
            <li>RCoT can significantly improve the pass@k performance of LLMs. With RCoT, 7 of 8 evaluated LLMs generated more efficient solutions, with most models achieving higher scores.</li>
          </p>
          <p>  
            <img width="782" alt="image" src="./images/pass_k.png">
            Performance comparison of models across PoT and RCoT tasks at different pass@k levels.
          </p>
          <p>  
            <li>The quality of reasoning significantly impacts the accuracy and efficiency of the model's final solution.</li>
          </p>
          <p>  
            <img width="782" alt="image" src="./images/self-reasoning.png">
            Performance comparison between self-reasoning and using GPT-4o reasoning for coding across different models. The results show that models perform better when relying on GPT-4o's reasoning output.
          </p>
          <p>
            We hope our findings contribute to a deeper understanding of current reasoning ability of LLMs and the further development of models.          
          </p>
        </div>
      </div>
    </div> 
  </div>
</section>



