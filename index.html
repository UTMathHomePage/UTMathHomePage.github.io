<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="UTMath: Math Evaluation with Unit Test
        via Reasoning-to-Coding Thoughts">
  <meta name="keywords" content="UTMath">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>UTMath: Math Evaluation with Unit Test
    via Reasoning-to-Coding Thoughts</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/shield.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">UTMath: Math Evaluation with Unit Test
            via Reasoning-to-Coding Thoughts</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Bo Yang<sup>1</sup>,</span>
            <span class="author-block">Qingping Yang,</span>
            <span class="author-block">Runtao Liu<sup>2</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>South China University of Technology,</span>
            <span class="author-block"><sup>2</sup>Hong Kong University of Science and Technology</span>
            
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">sdyangbo02 [at] mail [dot] scut [dot] edu [dot] cn </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2411.07240" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UTMathGroup/UTMath" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link -->
              <span class="link-block">
                <a href="https://github.com/UTMathGroup/UTMath/blob/main/data/utmath_problem.jsonl" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Hugging Face Link -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Leonardoby/UTMath" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">ü§ó</span>
                  <span>Hugging Face</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" style="margin-top: -60px; margin-bottom: -60px;">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
  <p align="center">
    <img alt="image" src="./images/overview.png">
  </p>
  <p><i>
    UTMath is a rigorous and expansive benchmark designed to evaluate the mathematical reasoning abilities of Large Language Models (LLMs), with an average of 68 test cases per problem to ensure that the model **truly solves the problem** rather than simply memorizing the answers. Based on sequences and inspired by the Online Encyclopedia of Integer Sequences ([OEIS](https://oeis.org/wiki/Welcome)), UTMath encompasses a diverse set of 1,053 problems across 9 distinct mathematical domains. This provides a comprehensive assessment of LLMs' capabilities. Drawing from unit testing methodologies in software development, UTMath‚Äôs innovative framework enables a detailed analysis of how well LLMs adapt and reason across a broad range of mathematical topics.
  </i></p>
  <p><i>
    The Reasoning-to-Coding of Thoughts (RCoT) approach complements the UTMath Benchmark by encouraging LLMs to engage in explicit reasoning prior to generating code. This approach fosters the development of more sophisticated and logically coherent solutions, leading to improved overall performance in mathematical problem-solving.
  </i></p>
  
  <ul>
    <li><b>‚ö°Ô∏èMultiple Case Validation</b>: Instead of using single cases that can be memorized, our questions are sequence-based, allowing numerous cases for validating true understanding.</li>
    <li><b>üîßTrue Reasoning Evaluation</b>: Hard cases and runtime metrics help filter memorization and compare solution efficiency, precisely assessing reasoning abilities.</li>
    <li><b>üèÜCode Output Evaluation</b>: We require LLMs to output code, focusing on reasoning rather than direct answers, to better reflect their reasoning skills.</li>
    <li><b>üåêObservation of Reasoning Process</b>: By mandating code implementation, we can observe and validate the LLM's reasoning process, not just the final answer.</li>
  </ul>
  </div>
  </div>
</section>

<section class="section" style="margin-top: -60px; margin-bottom: -60px;">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <p><b>[2024/11]</b>üöÄüöÄüöÄ: We released our <b>code</b>üìù
      <p><b>[2024/11]</b>: We released our benchmark <a href="https://github.com/UTMathGroup/UTMath/tree/main">UTMath</a>.</p>
    </div>
  </div>
</section>
<section class="section" id="BibTeX" style="margin-top: -50px; margin-bottom: -50px;">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">üí¨ Citation</h2>
    <p>
      If you find our work interesting and meaningful, welcome to give a üåü to our repo and cite our paper.
    </p>
    <pre><code>@misc{yang2024utmathmathevaluationunit,
      title={UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts}, 
      author={Bo Yang and Qingping Yang and Runtao Liu},
      year={2024},
      eprint={2411.07240},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.07240}, 
  }</code></pre>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
        <h2 class="title is-3">ü•∞ Acknowledgement</h2>
          <p>
            - We sincerely thank the <a href="https://oeis.org/wiki/Welcome" >OEIS</a> for its tireless efforts and contributions to the advancement of mathematics and computer science.
          </p>
          <p>
            - We are also grateful to <a href =https://github.com/openai/human-eval>HumanEval</a> for providing valuable code resources. 
          </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). 
            While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and adaptability. 
            This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests. It consists of 1,053 problems across 9 mathematical domains, with over 68 test cases per problem.
            We propose an innovative evaluation framework inspired by unit testing in software development, focusing on both accuracy and reliability of results. 
            Furthermore, we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to perform explicit reasoning before generating code, leading to generating more advanced solution and improved performance. Furthermore, we are releasing not only the UTMath benchmark but also the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.          </p>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ü•á Leaderboard</h2>
        <div class="content has-text-justified">
          <p align="center">
            <img width="774" alt="image" src="./images/leaderboard.png">
          </p>
          <p>
            Here, we consider the closed-source models, i.e., GPT-3.5-Turbo/GPT-4o from OpenAI, Claude-3.5-Sonnet, Gemini-1.5-Pro, as well as the open-source models, i.e., LLaMA-3.1, Qwen2.5, Qwen2.5-Math, DeepSeek-V2.5. The metric pass@1 is calculated as the average result over 5 run times. We run all evaluations in a laptop with CPU Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz.          
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üòé Some interesting findings</h2>
        <div class="content has-text-justified">
          <p>
            We conducted a comprehensive study with 8 LLMs. Some of our key findings are summarized as follows:
          </p>
          <p>
            - The best model, GPT-4o, only solves 26.93% problems in our benchmark, demonstrate the difficulty of our benchmarks.          
          </p>
          <p>  
            - Modern LLMs perform poorly in Graph Theory, Group Theory, Geometry and Topology.
          </p>
          <p align="center">
            <img width="782" alt="image" src="./images/performance_on_different_problems_categories.png">
          </p>
          <p>  
            - With RCoT, 7 of 8 evaluated LLMs generated more efficient solutions, with most models achieving higher scores.
          </p>
          <p>  
            - RCoT can significantly improve the pass@k performance of LLMs. 
          </p>
          <p>  
            <img width="782" alt="image" src="./images/pass_k.png">
          </p>
          <p>  
            - The quality of reasoning significantly impacts the accuracy and efficiency of the model's final solution.
          </p>
          <p>  
            <img width="782" alt="image" src="./images/self-reasoning.png">
          </p>
        </div>
      </div>
    </div>
    <p>
      We hope our findings contribute to a deeper understanding of current reasoning ability of LLMs and the further development of models.          
    </p>
  </div>
</section>



